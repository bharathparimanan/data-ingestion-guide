{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive Data Ingestion Guide\n",
        "\n",
        "## Overview\n",
        "This notebook provides a detailed and extensive guide to data ingestion methods, with a special focus on Kafka-based streaming ingestion. Each section includes self-explanatory inline comments and practical examples.\n",
        "\n",
        "### Topics Covered:\n",
        "1. **File-Based Ingestion** - CSV, JSON, Parquet, Excel\n",
        "2. **API-Based Ingestion** - REST APIs, Pagination, Rate Limiting\n",
        "3. **Database Ingestion** - PostgreSQL, MongoDB, SQL Server\n",
        "4. **IoT Ingestion** - MQTT, CoAP protocols\n",
        "5. **Kafka Streaming Ingestion** - Comprehensive guide with advanced patterns\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "- [Setup and Dependencies](#setup)\n",
        "- [File-Based Ingestion](#file-ingestion)\n",
        "- [API-Based Ingestion](#api-ingestion)\n",
        "- [Database Ingestion](#database-ingestion)\n",
        "- [IoT Ingestion](#iot-ingestion)\n",
        "- [Kafka Ingestion - Deep Dive](#kafka-ingestion)\n",
        "  - Kafka Fundamentals\n",
        "  - Producer Patterns\n",
        "  - Consumer Patterns\n",
        "  - Error Handling\n",
        "  - Schema Registry\n",
        "  - Advanced Patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Dependencies\n",
        "\n",
        "First, let's install and import all necessary libraries for data ingestion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports for data handling and utilities\n",
        "import json\n",
        "import csv\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Optional, Iterator, Callable\n",
        "from io import StringIO, BytesIO\n",
        "from collections import deque\n",
        "import asyncio\n",
        "\n",
        "# Data processing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Kafka libraries - confluent-kafka is the most popular Python client\n",
        "from confluent_kafka import Consumer, Producer, KafkaError, KafkaException\n",
        "from confluent_kafka.admin import AdminClient, NewTopic\n",
        "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
        "from confluent_kafka.schema_registry.avro import AvroSerializer, AvroDeserializer\n",
        "from confluent_kafka.schema_registry.json_schema import JSONSerializer, JSONDeserializer\n",
        "\n",
        "# API and HTTP libraries\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# Database libraries\n",
        "from sqlalchemy import create_engine, text\n",
        "import psycopg2\n",
        "from psycopg2.extras import RealDictCursor\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# IoT protocols\n",
        "from paho.mqtt import client as mqtt_client\n",
        "\n",
        "# Configure logging to see what's happening\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Create data directories for storing ingested data\n",
        "BASE_DATA_DIR = Path(\"./data\")\n",
        "BRONZE_DIR = BASE_DATA_DIR / \"bronze\"\n",
        "BRONZE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"✓ All dependencies imported successfully\")\n",
        "print(f\"✓ Data directory created: {BRONZE_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. File-Based Ingestion\n",
        "\n",
        "File-based ingestion is the most common method for batch data processing. We'll cover multiple file formats with error handling and metadata tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FileIngestion:\n",
        "    \"\"\"\n",
        "    Comprehensive file ingestion class supporting multiple formats.\n",
        "    Handles CSV, JSON, Parquet, Excel with automatic type detection and validation.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir: Path = BRONZE_DIR):\n",
        "        \"\"\"\n",
        "        Initialize file ingestion handler.\n",
        "        \n",
        "        Args:\n",
        "            output_dir: Directory where ingested files will be stored in Parquet format\n",
        "        \"\"\"\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.ingestion_stats = {\n",
        "            'files_processed': 0,\n",
        "            'records_ingested': 0,\n",
        "            'errors': []\n",
        "        }\n",
        "    \n",
        "    def ingest_file(\n",
        "        self,\n",
        "        file_path: str,\n",
        "        file_type: Optional[str] = None,\n",
        "        chunk_size: Optional[int] = None,\n",
        "        add_metadata: bool = True\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Ingest a single file and convert to Parquet format.\n",
        "        \n",
        "        Args:\n",
        "            file_path: Path to the source file\n",
        "            file_type: Explicit file type (csv, json, parquet, excel). Auto-detected if None\n",
        "            chunk_size: Process file in chunks if specified (useful for large files)\n",
        "            add_metadata: Whether to add ingestion metadata columns\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with ingestion statistics and output file path\n",
        "        \"\"\"\n",
        "        file_path = Path(file_path)\n",
        "        \n",
        "        # Auto-detect file type from extension if not specified\n",
        "        if file_type is None:\n",
        "            file_type = self._detect_file_type(file_path)\n",
        "        \n",
        "        logger.info(f\"Ingesting file: {file_path} (type: {file_type})\")\n",
        "        \n",
        "        try:\n",
        "            # Read file based on type\n",
        "            # Using chunking for large files to avoid memory issues\n",
        "            if chunk_size:\n",
        "                return self._ingest_file_chunked(file_path, file_type, chunk_size, add_metadata)\n",
        "            else:\n",
        "                df = self._read_file(file_path, file_type)\n",
        "                \n",
        "                # Add metadata columns for lineage tracking\n",
        "                if add_metadata:\n",
        "                    df = self._add_metadata(df, file_path, file_type)\n",
        "                \n",
        "                # Write to Parquet format (columnar storage, efficient compression)\n",
        "                output_path = self._write_to_parquet(df, file_path.stem)\n",
        "                \n",
        "                # Update statistics\n",
        "                self.ingestion_stats['files_processed'] += 1\n",
        "                self.ingestion_stats['records_ingested'] += len(df)\n",
        "                \n",
        "                return {\n",
        "                    'success': True,\n",
        "                    'records': len(df),\n",
        "                    'output_path': str(output_path),\n",
        "                    'file_type': file_type\n",
        "                }\n",
        "                \n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error ingesting {file_path}: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            self.ingestion_stats['errors'].append(error_msg)\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "    \n",
        "    def _read_file(self, file_path: Path, file_type: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Read file into pandas DataFrame based on file type.\n",
        "        \n",
        "        This method handles different file formats with appropriate pandas readers.\n",
        "        \"\"\"\n",
        "        if file_type == 'csv':\n",
        "            # CSV reading with common options\n",
        "            # low_memory=False ensures consistent dtypes\n",
        "            # encoding='utf-8' handles special characters\n",
        "            return pd.read_csv(\n",
        "                file_path,\n",
        "                low_memory=False,\n",
        "                encoding='utf-8',\n",
        "                on_bad_lines='skip'  # Skip malformed lines instead of failing\n",
        "            )\n",
        "        \n",
        "        elif file_type == 'json':\n",
        "            # JSON can be single object, array, or JSONL (newline-delimited)\n",
        "            # Try JSONL first (common for streaming data)\n",
        "            try:\n",
        "                return pd.read_json(file_path, lines=True)\n",
        "            except:\n",
        "                # Fall back to regular JSON\n",
        "                return pd.read_json(file_path)\n",
        "        \n",
        "        elif file_type == 'parquet':\n",
        "            # Parquet is already columnar, just read it\n",
        "            return pd.read_parquet(file_path, engine='pyarrow')\n",
        "        \n",
        "        elif file_type == 'excel':\n",
        "            # Excel files can have multiple sheets\n",
        "            # Read first sheet by default\n",
        "            return pd.read_excel(file_path, engine='openpyxl')\n",
        "        \n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file type: {file_type}\")\n",
        "    \n",
        "    def _detect_file_type(self, file_path: Path) -> str:\n",
        "        \"\"\"Detect file type from extension.\"\"\"\n",
        "        ext = file_path.suffix.lower()\n",
        "        mapping = {\n",
        "            '.csv': 'csv',\n",
        "            '.json': 'json',\n",
        "            '.jsonl': 'json',  # JSON Lines format\n",
        "            '.parquet': 'parquet',\n",
        "            '.xlsx': 'excel',\n",
        "            '.xls': 'excel'\n",
        "        }\n",
        "        return mapping.get(ext, 'csv')  # Default to CSV if unknown\n",
        "    \n",
        "    def _add_metadata(self, df: pd.DataFrame, source_path: Path, file_type: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Add metadata columns for data lineage and tracking.\n",
        "        \n",
        "        These columns help track:\n",
        "        - When data was ingested\n",
        "        - Source file information\n",
        "        - Processing timestamps\n",
        "        \"\"\"\n",
        "        df = df.copy()  # Avoid modifying original\n",
        "        \n",
        "        # Ingestion timestamp (when this data was loaded)\n",
        "        df['_ingestion_timestamp'] = datetime.utcnow().isoformat()\n",
        "        \n",
        "        # Source file information\n",
        "        df['_source_file'] = source_path.name\n",
        "        df['_source_path'] = str(source_path)\n",
        "        df['_source_type'] = 'file'\n",
        "        df['_file_type'] = file_type\n",
        "        \n",
        "        # Record count for validation\n",
        "        df['_record_id'] = range(len(df))\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _write_to_parquet(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        base_name: str,\n",
        "        partition_by_date: bool = True\n",
        "    ) -> Path:\n",
        "        \"\"\"\n",
        "        Write DataFrame to Parquet format with optimizations.\n",
        "        \n",
        "        Parquet benefits:\n",
        "        - Columnar storage (efficient for analytics)\n",
        "        - Compression (saves storage space)\n",
        "        - Schema preservation (data types maintained)\n",
        "        - Partitioning support (for time-series data)\n",
        "        \"\"\"\n",
        "        # Generate timestamped filename\n",
        "        timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"{base_name}_{timestamp}.parquet\"\n",
        "        \n",
        "        # Create output path\n",
        "        output_path = self.output_dir / filename\n",
        "        \n",
        "        # Partition by date if timestamp column exists\n",
        "        # This improves query performance for time-based filtering\n",
        "        if partition_by_date and '_ingestion_timestamp' in df.columns:\n",
        "            # Extract date from timestamp for partitioning\n",
        "            df['_date'] = pd.to_datetime(df['_ingestion_timestamp']).dt.date\n",
        "            \n",
        "            # Write with partitioning (creates subdirectories by date)\n",
        "            df.to_parquet(\n",
        "                output_path.parent,\n",
        "                engine='pyarrow',\n",
        "                compression='snappy',  # Fast compression algorithm\n",
        "                partition_cols=['_date'],\n",
        "                index=False,\n",
        "                schema=self._infer_schema(df)\n",
        "            )\n",
        "        else:\n",
        "            # Write single file\n",
        "            df.to_parquet(\n",
        "                output_path,\n",
        "                engine='pyarrow',\n",
        "                compression='snappy',\n",
        "                index=False,\n",
        "                schema=self._infer_schema(df)\n",
        "            )\n",
        "        \n",
        "        logger.info(f\"Wrote {len(df)} records to {output_path}\")\n",
        "        return output_path\n",
        "    \n",
        "    def _infer_schema(self, df: pd.DataFrame) -> Optional[pa.Schema]:\n",
        "        \"\"\"\n",
        "        Infer PyArrow schema from DataFrame.\n",
        "        \n",
        "        Schema inference ensures:\n",
        "        - Consistent data types across files\n",
        "        - Better compression\n",
        "        - Query optimization\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return pa.Schema.from_pandas(df)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not infer schema: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def _ingest_file_chunked(\n",
        "        self,\n",
        "        file_path: Path,\n",
        "        file_type: str,\n",
        "        chunk_size: int,\n",
        "        add_metadata: bool\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process large files in chunks to avoid memory issues.\n",
        "        \n",
        "        This is essential for files larger than available RAM.\n",
        "        Each chunk is processed and written separately.\n",
        "        \"\"\"\n",
        "        total_records = 0\n",
        "        chunk_files = []\n",
        "        \n",
        "        # Read file in chunks\n",
        "        if file_type == 'csv':\n",
        "            chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size)\n",
        "        elif file_type == 'json':\n",
        "            # JSON chunking is more complex, read full file for now\n",
        "            df = self._read_file(file_path, file_type)\n",
        "            chunk_iterator = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
        "        else:\n",
        "            # For other types, process normally\n",
        "            return self.ingest_file(str(file_path), file_type, chunk_size=None)\n",
        "        \n",
        "        # Process each chunk\n",
        "        for chunk_num, chunk_df in enumerate(chunk_iterator):\n",
        "            if add_metadata:\n",
        "                chunk_df = self._add_metadata(chunk_df, file_path, file_type)\n",
        "            \n",
        "            output_path = self._write_to_parquet(\n",
        "                chunk_df,\n",
        "                f\"{file_path.stem}_chunk{chunk_num}\"\n",
        "            )\n",
        "            chunk_files.append(str(output_path))\n",
        "            total_records += len(chunk_df)\n",
        "        \n",
        "        return {\n",
        "            'success': True,\n",
        "            'records': total_records,\n",
        "            'chunk_files': chunk_files,\n",
        "            'file_type': file_type\n",
        "        }\n",
        "\n",
        "# Example usage\n",
        "file_ingestion = FileIngestion()\n",
        "print(\"✓ File ingestion class initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Create sample CSV file for demonstration\n",
        "sample_data = {\n",
        "    'customer_id': [1, 2, 3, 4, 5],\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
        "    'email': ['alice@example.com', 'bob@example.com', 'charlie@example.com', \n",
        "              'diana@example.com', 'eve@example.com'],\n",
        "    'purchase_amount': [100.50, 250.75, 89.99, 150.00, 300.25],\n",
        "    'purchase_date': ['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19']\n",
        "}\n",
        "\n",
        "sample_df = pd.DataFrame(sample_data)\n",
        "sample_csv_path = BRONZE_DIR / \"sample_customers.csv\"\n",
        "sample_df.to_csv(sample_csv_path, index=False)\n",
        "\n",
        "# Ingest the CSV file\n",
        "result = file_ingestion.ingest_file(str(sample_csv_path))\n",
        "print(f\"\\nIngestion Result:\")\n",
        "print(json.dumps(result, indent=2))\n",
        "\n",
        "# Verify the output\n",
        "if result['success']:\n",
        "    output_df = pd.read_parquet(result['output_path'])\n",
        "    print(f\"\\n✓ Successfully ingested {result['records']} records\")\n",
        "    print(f\"\\nOutput DataFrame columns: {list(output_df.columns)}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    print(output_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. API-Based Ingestion\n",
        "\n",
        "REST API ingestion with support for pagination, rate limiting, authentication, and error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class APIIngestion:\n",
        "    \"\"\"\n",
        "    Comprehensive API ingestion class with pagination, rate limiting, and retry logic.\n",
        "    \n",
        "    Handles:\n",
        "    - REST API calls with authentication\n",
        "    - Pagination (offset-based, cursor-based, page-based)\n",
        "    - Rate limiting and backoff strategies\n",
        "    - Error handling and retries\n",
        "    - Data transformation and validation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir: Path = BRONZE_DIR):\n",
        "        \"\"\"Initialize API ingestion handler.\"\"\"\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Configure requests session with retry strategy\n",
        "        # This handles transient network errors automatically\n",
        "        self.session = requests.Session()\n",
        "        retry_strategy = Retry(\n",
        "            total=3,  # Maximum number of retries\n",
        "            backoff_factor=1,  # Wait 1, 2, 4 seconds between retries\n",
        "            status_forcelist=[429, 500, 502, 503, 504],  # Retry on these HTTP status codes\n",
        "            allowed_methods=[\"GET\", \"POST\"]  # Only retry safe methods\n",
        "        )\n",
        "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "        self.session.mount(\"http://\", adapter)\n",
        "        self.session.mount(\"https://\", adapter)\n",
        "    \n",
        "    def ingest_from_api(\n",
        "        self,\n",
        "        url: str,\n",
        "        method: str = \"GET\",\n",
        "        headers: Optional[Dict[str, str]] = None,\n",
        "        params: Optional[Dict[str, Any]] = None,\n",
        "        auth: Optional[tuple] = None,\n",
        "        pagination_config: Optional[Dict[str, Any]] = None,\n",
        "        rate_limit_delay: float = 0.0,\n",
        "        max_records: Optional[int] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Ingest data from REST API endpoint.\n",
        "        \n",
        "        Args:\n",
        "            url: API endpoint URL\n",
        "            method: HTTP method (GET, POST, etc.)\n",
        "            headers: HTTP headers (for authentication tokens, content-type, etc.)\n",
        "            params: Query parameters\n",
        "            auth: Basic auth tuple (username, password)\n",
        "            pagination_config: Configuration for handling paginated responses\n",
        "            rate_limit_delay: Seconds to wait between requests (rate limiting)\n",
        "            max_records: Maximum number of records to ingest (safety limit)\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with ingestion statistics\n",
        "        \"\"\"\n",
        "        logger.info(f\"Ingesting from API: {url}\")\n",
        "        \n",
        "        all_records = []\n",
        "        request_count = 0\n",
        "        start_time = datetime.utcnow()\n",
        "        \n",
        "        try:\n",
        "            # Handle paginated APIs\n",
        "            if pagination_config:\n",
        "                all_records = self._fetch_paginated(\n",
        "                    url, method, headers, params, auth, pagination_config, \n",
        "                    rate_limit_delay, max_records\n",
        "                )\n",
        "            else:\n",
        "                # Single request API\n",
        "                response = self._make_request(url, method, headers, params, auth)\n",
        "                records = self._extract_records(response.json())\n",
        "                all_records.extend(records)\n",
        "            \n",
        "            # Convert to DataFrame\n",
        "            if not all_records:\n",
        "                logger.warning(\"No records retrieved from API\")\n",
        "                return {'success': False, 'error': 'No records found'}\n",
        "            \n",
        "            df = pd.DataFrame(all_records)\n",
        "            \n",
        "            # Add metadata\n",
        "            df['_ingestion_timestamp'] = datetime.utcnow().isoformat()\n",
        "            df['_source_api'] = url\n",
        "            df['_source_type'] = 'api'\n",
        "            df['_api_method'] = method\n",
        "            \n",
        "            # Write to Parquet\n",
        "            timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            safe_url = \"\".join(c for c in url.split(\"//\")[-1] if c.isalnum() or c in ('-', '_', '.'))[:50]\n",
        "            output_path = self.output_dir / f\"api_{safe_url}_{timestamp}.parquet\"\n",
        "            \n",
        "            df.to_parquet(output_path, engine='pyarrow', compression='snappy', index=False)\n",
        "            \n",
        "            duration = (datetime.utcnow() - start_time).total_seconds()\n",
        "            \n",
        "            return {\n",
        "                'success': True,\n",
        "                'records': len(df),\n",
        "                'output_path': str(output_path),\n",
        "                'duration_seconds': duration,\n",
        "                'api_url': url\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error ingesting from API: {str(e)}\")\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "    \n",
        "    def _make_request(\n",
        "        self,\n",
        "        url: str,\n",
        "        method: str,\n",
        "        headers: Optional[Dict],\n",
        "        params: Optional[Dict],\n",
        "        auth: Optional[tuple]\n",
        "    ) -> requests.Response:\n",
        "        \"\"\"\n",
        "        Make HTTP request with error handling.\n",
        "        \n",
        "        This method centralizes request logic and error handling.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = self.session.request(\n",
        "                method=method,\n",
        "                url=url,\n",
        "                headers=headers,\n",
        "                params=params,\n",
        "                auth=auth,\n",
        "                timeout=30  # 30 second timeout\n",
        "            )\n",
        "            \n",
        "            # Raise exception for HTTP errors (4xx, 5xx)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Request failed: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    def _extract_records(self, data: Any) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extract records from API response.\n",
        "        \n",
        "        APIs return data in different formats:\n",
        "        - Direct array: [{\"id\": 1}, {\"id\": 2}]\n",
        "        - Wrapped object: {\"data\": [{\"id\": 1}]}\n",
        "        - Single object: {\"id\": 1}\n",
        "        \n",
        "        This method handles all common formats.\n",
        "        \"\"\"\n",
        "        if isinstance(data, list):\n",
        "            # Direct array response\n",
        "            return data\n",
        "        elif isinstance(data, dict):\n",
        "            # Try common keys that wrap arrays\n",
        "            for key in ['data', 'results', 'items', 'records', 'content']:\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            # Single object - wrap in list\n",
        "            return [data]\n",
        "        else:\n",
        "            return []\n",
        "    \n",
        "    def _fetch_paginated(\n",
        "        self,\n",
        "        url: str,\n",
        "        method: str,\n",
        "        headers: Optional[Dict],\n",
        "        params: Optional[Dict],\n",
        "        auth: Optional[tuple],\n",
        "        pagination_config: Dict[str, Any],\n",
        "        rate_limit_delay: float,\n",
        "        max_records: Optional[int]\n",
        "    ) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Handle paginated API responses.\n",
        "        \n",
        "        Supports multiple pagination strategies:\n",
        "        1. Offset-based: ?offset=0&limit=100\n",
        "        2. Page-based: ?page=1&size=100\n",
        "        3. Cursor-based: ?cursor=abc123\n",
        "        \n",
        "        This method automatically handles all pages until:\n",
        "        - No more data is returned\n",
        "        - Maximum records limit is reached\n",
        "        - Maximum pages limit is reached\n",
        "        \"\"\"\n",
        "        all_records = []\n",
        "        pagination_type = pagination_config.get('type', 'offset')  # offset, page, cursor\n",
        "        \n",
        "        # Pagination parameters\n",
        "        page_param = pagination_config.get('page_param', 'page')\n",
        "        size_param = pagination_config.get('size_param', 'size')\n",
        "        offset_param = pagination_config.get('offset_param', 'offset')\n",
        "        limit_param = pagination_config.get('limit_param', 'limit')\n",
        "        page_size = pagination_config.get('page_size', 100)\n",
        "        max_pages = pagination_config.get('max_pages', 1000)\n",
        "        data_key = pagination_config.get('data_key', 'data')  # Key containing records in response\n",
        "        has_more_key = pagination_config.get('has_more_key')  # Key indicating more pages\n",
        "        \n",
        "        # Initialize pagination state\n",
        "        if pagination_type == 'offset':\n",
        "            offset = pagination_config.get('start_offset', 0)\n",
        "        elif pagination_type == 'page':\n",
        "            page = pagination_config.get('start_page', 1)\n",
        "        elif pagination_type == 'cursor':\n",
        "            cursor = pagination_config.get('start_cursor')\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported pagination type: {pagination_type}\")\n",
        "        \n",
        "        current_page = 0\n",
        "        \n",
        "        while current_page < max_pages:\n",
        "            # Prepare request parameters\n",
        "            request_params = params.copy() if params else {}\n",
        "            \n",
        "            if pagination_type == 'offset':\n",
        "                request_params[offset_param] = offset\n",
        "                request_params[limit_param] = page_size\n",
        "            elif pagination_type == 'page':\n",
        "                request_params[page_param] = page\n",
        "                request_params[size_param] = page_size\n",
        "            elif pagination_type == 'cursor':\n",
        "                if cursor:\n",
        "                    request_params['cursor'] = cursor\n",
        "            \n",
        "            # Make request\n",
        "            response = self._make_request(url, method, headers, request_params, auth)\n",
        "            response_data = response.json()\n",
        "            \n",
        "            # Extract records from response\n",
        "            records = self._extract_records(response_data)\n",
        "            \n",
        "            if not records:\n",
        "                # No more data\n",
        "                break\n",
        "            \n",
        "            all_records.extend(records)\n",
        "            current_page += 1\n",
        "            \n",
        "            # Check if we've reached max records limit\n",
        "            if max_records and len(all_records) >= max_records:\n",
        "                all_records = all_records[:max_records]\n",
        "                break\n",
        "            \n",
        "            # Check if more pages exist (for APIs that provide this info)\n",
        "            if has_more_key:\n",
        "                if not response_data.get(has_more_key, False):\n",
        "                    break\n",
        "            \n",
        "            # Update pagination state\n",
        "            if pagination_type == 'offset':\n",
        "                offset += page_size\n",
        "            elif pagination_type == 'page':\n",
        "                page += 1\n",
        "            elif pagination_type == 'cursor':\n",
        "                # Cursor is typically in response\n",
        "                cursor = response_data.get('next_cursor') or response_data.get('cursor')\n",
        "                if not cursor:\n",
        "                    break\n",
        "            \n",
        "            # Rate limiting - wait between requests to avoid overwhelming API\n",
        "            if rate_limit_delay > 0:\n",
        "                time.sleep(rate_limit_delay)\n",
        "        \n",
        "        logger.info(f\"Fetched {len(all_records)} records across {current_page} pages\")\n",
        "        return all_records\n",
        "\n",
        "# Initialize API ingestion\n",
        "api_ingestion = APIIngestion()\n",
        "print(\"✓ API ingestion class initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Database Ingestion\n",
        "\n",
        "Ingest data from relational databases (PostgreSQL, MySQL) and NoSQL databases (MongoDB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DatabaseIngestion:\n",
        "    \"\"\"\n",
        "    Database ingestion class supporting both SQL and NoSQL databases.\n",
        "    \n",
        "    Features:\n",
        "    - SQL databases: PostgreSQL, MySQL, SQL Server via SQLAlchemy\n",
        "    - NoSQL databases: MongoDB\n",
        "    - Incremental loading with checkpoint tracking\n",
        "    - Query optimization with chunking\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir: Path = BRONZE_DIR):\n",
        "        \"\"\"Initialize database ingestion handler.\"\"\"\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    def ingest_from_sql(\n",
        "        self,\n",
        "        connection_string: str,\n",
        "        query: str,\n",
        "        database_type: str = \"postgresql\",\n",
        "        chunk_size: Optional[int] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Ingest data from SQL database using SQLAlchemy.\n",
        "        \n",
        "        Args:\n",
        "            connection_string: Database connection string\n",
        "                Example: \"postgresql://user:password@localhost:5432/dbname\"\n",
        "            query: SQL query to execute\n",
        "            database_type: Type of database (postgresql, mysql, sqlite, etc.)\n",
        "            chunk_size: Process query results in chunks if specified\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with ingestion statistics\n",
        "        \"\"\"\n",
        "        logger.info(f\"Ingesting from {database_type} database\")\n",
        "        \n",
        "        try:\n",
        "            # Create SQLAlchemy engine\n",
        "            # Engine manages connection pooling for efficiency\n",
        "            engine = create_engine(connection_string, pool_pre_ping=True)\n",
        "            \n",
        "            # Execute query\n",
        "            if chunk_size:\n",
        "                # Process in chunks for large result sets\n",
        "                chunks = []\n",
        "                for chunk_df in pd.read_sql(query, engine, chunksize=chunk_size):\n",
        "                    chunks.append(chunk_df)\n",
        "                df = pd.concat(chunks, ignore_index=True)\n",
        "            else:\n",
        "                # Load all data at once\n",
        "                df = pd.read_sql(query, engine)\n",
        "            \n",
        "            # Add metadata\n",
        "            df['_ingestion_timestamp'] = datetime.utcnow().isoformat()\n",
        "            df['_source_type'] = 'database'\n",
        "            df['_database_type'] = database_type\n",
        "            \n",
        "            # Write to Parquet\n",
        "            timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            output_path = self.output_dir / f\"db_{database_type}_{timestamp}.parquet\"\n",
        "            \n",
        "            df.to_parquet(output_path, engine='pyarrow', compression='snappy', index=False)\n",
        "            \n",
        "            return {\n",
        "                'success': True,\n",
        "                'records': len(df),\n",
        "                'output_path': str(output_path),\n",
        "                'database_type': database_type\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error ingesting from database: {str(e)}\")\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "    \n",
        "    def ingest_from_mongodb(\n",
        "        self,\n",
        "        connection_string: str,\n",
        "        database_name: str,\n",
        "        collection_name: str,\n",
        "        query: Optional[Dict] = None,\n",
        "        projection: Optional[Dict] = None,\n",
        "        limit: Optional[int] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Ingest data from MongoDB collection.\n",
        "        \n",
        "        Args:\n",
        "            connection_string: MongoDB connection string\n",
        "                Example: \"mongodb://localhost:27017/\"\n",
        "            database_name: Name of MongoDB database\n",
        "            collection_name: Name of collection to query\n",
        "            query: MongoDB query filter (dict)\n",
        "            projection: Fields to include/exclude (dict)\n",
        "            limit: Maximum number of documents to retrieve\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with ingestion statistics\n",
        "        \"\"\"\n",
        "        logger.info(f\"Ingesting from MongoDB: {database_name}.{collection_name}\")\n",
        "        \n",
        "        try:\n",
        "            # Connect to MongoDB\n",
        "            client = MongoClient(connection_string)\n",
        "            db = client[database_name]\n",
        "            collection = db[collection_name]\n",
        "            \n",
        "            # Build query\n",
        "            cursor = collection.find(query or {}, projection)\n",
        "            \n",
        "            if limit:\n",
        "                cursor = cursor.limit(limit)\n",
        "            \n",
        "            # Convert to list of dictionaries\n",
        "            documents = list(cursor)\n",
        "            \n",
        "            if not documents:\n",
        "                return {'success': False, 'error': 'No documents found'}\n",
        "            \n",
        "            # Convert to DataFrame\n",
        "            # MongoDB documents are already dictionaries\n",
        "            df = pd.DataFrame(documents)\n",
        "            \n",
        "            # Add metadata\n",
        "            df['_ingestion_timestamp'] = datetime.utcnow().isoformat()\n",
        "            df['_source_type'] = 'database'\n",
        "            df['_database_type'] = 'mongodb'\n",
        "            df['_collection'] = collection_name\n",
        "            \n",
        "            # Write to Parquet\n",
        "            timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            output_path = self.output_dir / f\"db_mongodb_{collection_name}_{timestamp}.parquet\"\n",
        "            \n",
        "            df.to_parquet(output_path, engine='pyarrow', compression='snappy', index=False)\n",
        "            \n",
        "            client.close()\n",
        "            \n",
        "            return {\n",
        "                'success': True,\n",
        "                'records': len(df),\n",
        "                'output_path': str(output_path),\n",
        "                'database_type': 'mongodb'\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error ingesting from MongoDB: {str(e)}\")\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "# Initialize database ingestion\n",
        "db_ingestion = DatabaseIngestion()\n",
        "print(\"✓ Database ingestion class initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IoTIngestion:\n",
        "    \"\"\"\n",
        "    IoT device ingestion using MQTT protocol.\n",
        "    \n",
        "    MQTT (Message Queuing Telemetry Transport) is a lightweight messaging protocol\n",
        "    designed for IoT devices with limited bandwidth and resources.\n",
        "    \n",
        "    Features:\n",
        "    - Subscribe to MQTT topics\n",
        "    - Collect messages over time period\n",
        "    - Handle QoS levels (Quality of Service)\n",
        "    - Authentication support\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir: Path = BRONZE_DIR):\n",
        "        \"\"\"Initialize IoT ingestion handler.\"\"\"\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.messages = []  # Buffer for collected messages\n",
        "    \n",
        "    def ingest_from_mqtt(\n",
        "        self,\n",
        "        broker: str,\n",
        "        port: int = 1883,\n",
        "        topics: List[str] = None,\n",
        "        username: Optional[str] = None,\n",
        "        password: Optional[str] = None,\n",
        "        duration_seconds: int = 60,\n",
        "        qos: int = 1\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Ingest data from MQTT broker.\n",
        "        \n",
        "        Args:\n",
        "            broker: MQTT broker hostname or IP\n",
        "            port: MQTT broker port (default 1883, TLS usually 8883)\n",
        "            topics: List of topics to subscribe to (use ['#'] for all topics)\n",
        "            username: Optional username for authentication\n",
        "            password: Optional password for authentication\n",
        "            duration_seconds: How long to collect messages\n",
        "            qos: Quality of Service level (0=at most once, 1=at least once, 2=exactly once)\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with ingestion statistics\n",
        "        \"\"\"\n",
        "        logger.info(f\"Connecting to MQTT broker: {broker}:{port}\")\n",
        "        \n",
        "        self.messages = []  # Reset message buffer\n",
        "        \n",
        "        # MQTT client callback functions\n",
        "        def on_connect(client, userdata, flags, rc):\n",
        "            \"\"\"\n",
        "            Callback when connection is established.\n",
        "            rc (return code):\n",
        "            0: Connection successful\n",
        "            1: Connection refused - incorrect protocol version\n",
        "            2: Connection refused - invalid client identifier\n",
        "            3: Connection refused - server unavailable\n",
        "            4: Connection refused - bad username or password\n",
        "            5: Connection refused - not authorized\n",
        "            \"\"\"\n",
        "            if rc == 0:\n",
        "                logger.info(\"Connected to MQTT broker\")\n",
        "                # Subscribe to topics after connection\n",
        "                topics_to_subscribe = topics or ['#']  # '#' subscribes to all topics\n",
        "                for topic in topics_to_subscribe:\n",
        "                    client.subscribe(topic, qos=qos)\n",
        "                    logger.info(f\"Subscribed to topic: {topic}\")\n",
        "            else:\n",
        "                logger.error(f\"Failed to connect to MQTT broker, return code: {rc}\")\n",
        "        \n",
        "        def on_message(client, userdata, msg):\n",
        "            \"\"\"\n",
        "            Callback when message is received.\n",
        "            This function is called for every message received on subscribed topics.\n",
        "            \"\"\"\n",
        "            try:\n",
        "                # Decode message payload\n",
        "                # MQTT messages are typically JSON, but can be any format\n",
        "                payload_str = msg.payload.decode('utf-8')\n",
        "                \n",
        "                # Try to parse as JSON\n",
        "                try:\n",
        "                    payload = json.loads(payload_str)\n",
        "                except json.JSONDecodeError:\n",
        "                    # If not JSON, store as raw string\n",
        "                    payload = {'_raw_message': payload_str}\n",
        "                \n",
        "                # Add MQTT metadata\n",
        "                message_record = {\n",
        "                    **payload,\n",
        "                    '_mqtt_topic': msg.topic,  # Topic the message was received on\n",
        "                    '_mqtt_qos': msg.qos,  # Quality of Service level\n",
        "                    '_mqtt_retain': msg.retain,  # Retain flag\n",
        "                    '_mqtt_timestamp': datetime.utcnow().isoformat()  # When we received it\n",
        "                }\n",
        "                \n",
        "                # Store message\n",
        "                self.messages.append(message_record)\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing MQTT message: {str(e)}\")\n",
        "        \n",
        "        def on_disconnect(client, userdata, rc):\n",
        "            \"\"\"Callback when disconnected from broker.\"\"\"\n",
        "            logger.info(\"Disconnected from MQTT broker\")\n",
        "        \n",
        "        # Create MQTT client\n",
        "        # Client ID must be unique - using timestamp to ensure uniqueness\n",
        "        client_id = f\"bronze_ingestion_{int(time.time())}\"\n",
        "        client = mqtt_client.Client(client_id=client_id)\n",
        "        \n",
        "        # Set authentication if provided\n",
        "        if username:\n",
        "            client.username_pw_set(username, password)\n",
        "        \n",
        "        # Set callback functions\n",
        "        client.on_connect = on_connect\n",
        "        client.on_message = on_message\n",
        "        client.on_disconnect = on_disconnect\n",
        "        \n",
        "        try:\n",
        "            # Connect to broker\n",
        "            client.connect(broker, port, keepalive=60)\n",
        "            \n",
        "            # Start network loop in background thread\n",
        "            # This handles incoming messages asynchronously\n",
        "            client.loop_start()\n",
        "            \n",
        "            # Collect messages for specified duration\n",
        "            logger.info(f\"Collecting messages for {duration_seconds} seconds...\")\n",
        "            time.sleep(duration_seconds)\n",
        "            \n",
        "            # Stop network loop\n",
        "            client.loop_stop()\n",
        "            client.disconnect()\n",
        "            \n",
        "            # Process collected messages\n",
        "            if not self.messages:\n",
        "                return {\n",
        "                    'success': False,\n",
        "                    'error': 'No messages received'\n",
        "                }\n",
        "            \n",
        "            # Convert to DataFrame\n",
        "            df = pd.DataFrame(self.messages)\n",
        "            \n",
        "            # Add metadata\n",
        "            df['_ingestion_timestamp'] = datetime.utcnow().isoformat()\n",
        "            df['_source_type'] = 'iot'\n",
        "            df['_iot_protocol'] = 'mqtt'\n",
        "            df['_broker'] = broker\n",
        "            \n",
        "            # Write to Parquet\n",
        "            timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            output_path = self.output_dir / f\"iot_mqtt_{timestamp}.parquet\"\n",
        "            \n",
        "            df.to_parquet(output_path, engine='pyarrow', compression='snappy', index=False)\n",
        "            \n",
        "            return {\n",
        "                'success': True,\n",
        "                'records': len(df),\n",
        "                'output_path': str(output_path),\n",
        "                'topics': topics or ['#'],\n",
        "                'duration_seconds': duration_seconds\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error ingesting from MQTT: {str(e)}\")\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "# Initialize IoT ingestion\n",
        "iot_ingestion = IoTIngestion()\n",
        "print(\"✓ IoT ingestion class initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Kafka Ingestion - Comprehensive Guide\n",
        "\n",
        "Kafka is a distributed streaming platform designed for high-throughput, fault-tolerant, real-time data pipelines. This section provides an extensive guide to Kafka ingestion with detailed explanations.\n",
        "\n",
        "### Kafka Fundamentals\n",
        "\n",
        "**Key Concepts:**\n",
        "- **Topic**: Category/feed name where records are published\n",
        "- **Partition**: Topics are split into partitions for parallelism and scalability\n",
        "- **Producer**: Applications that publish data to topics\n",
        "- **Consumer**: Applications that read data from topics\n",
        "- **Consumer Group**: Group of consumers working together to consume a topic\n",
        "- **Offset**: Position of a consumer in a partition (like a bookmark)\n",
        "- **Broker**: Kafka server that stores data and serves clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KafkaIngestion:\n",
        "    \"\"\"\n",
        "    Comprehensive Kafka ingestion class with advanced features.\n",
        "    \n",
        "    This class demonstrates:\n",
        "    1. Kafka Producer - Publishing data to topics\n",
        "    2. Kafka Consumer - Consuming data from topics\n",
        "    3. Consumer Groups - Parallel processing\n",
        "    4. Offset Management - Resuming from specific positions\n",
        "    5. Error Handling - Retries and dead letter queues\n",
        "    6. Schema Registry - Avro and JSON schema support\n",
        "    7. Streaming to Parquet - Batch writing for analytics\n",
        "    8. Exactly-once semantics - Idempotent producers\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        bootstrap_servers: str = \"localhost:9092\",\n",
        "        schema_registry_url: Optional[str] = None,\n",
        "        output_dir: Path = BRONZE_DIR\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize Kafka ingestion handler.\n",
        "        \n",
        "        Args:\n",
        "            bootstrap_servers: Comma-separated list of Kafka broker addresses\n",
        "                Example: \"localhost:9092\" or \"broker1:9092,broker2:9092\"\n",
        "            schema_registry_url: URL of Confluent Schema Registry (optional)\n",
        "                Used for Avro and JSON schema serialization\n",
        "            output_dir: Directory for storing ingested data\n",
        "        \"\"\"\n",
        "        self.bootstrap_servers = bootstrap_servers\n",
        "        self.schema_registry_url = schema_registry_url\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Initialize Schema Registry client if URL provided\n",
        "        # Schema Registry stores schemas and enables schema evolution\n",
        "        self.schema_registry_client = None\n",
        "        if schema_registry_url:\n",
        "            try:\n",
        "                self.schema_registry_client = SchemaRegistryClient({\n",
        "                    'url': schema_registry_url\n",
        "                })\n",
        "                logger.info(f\"Connected to Schema Registry: {schema_registry_url}\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not connect to Schema Registry: {e}\")\n",
        "        \n",
        "        # Statistics tracking\n",
        "        self.stats = {\n",
        "            'messages_produced': 0,\n",
        "            'messages_consumed': 0,\n",
        "            'batches_written': 0,\n",
        "            'errors': []\n",
        "        }\n",
        "    \n",
        "    # ========================================================================\n",
        "    # KAFKA PRODUCER METHODS\n",
        "    # ========================================================================\n",
        "    \n",
        "    def create_producer(\n",
        "        self,\n",
        "        acks: str = \"all\",\n",
        "        retries: int = 3,\n",
        "        enable_idempotence: bool = True,\n",
        "        max_in_flight_requests_per_connection: int = 5,\n",
        "        compression_type: str = \"snappy\"\n",
        "    ) -> Producer:\n",
        "        \"\"\"\n",
        "        Create and configure Kafka Producer.\n",
        "        \n",
        "        Producer Configuration Explained:\n",
        "        \n",
        "        acks (acknowledgment):\n",
        "        - \"0\": Fire and forget (no acknowledgment, fastest but least reliable)\n",
        "        - \"1\": Wait for leader acknowledgment (good balance)\n",
        "        - \"all\" or \"-1\": Wait for all in-sync replicas (most reliable, slowest)\n",
        "        \n",
        "        retries:\n",
        "        - Number of times to retry failed sends\n",
        "        - Set to 0 to disable retries\n",
        "        \n",
        "        enable_idempotence:\n",
        "        - Ensures exactly-once semantics (no duplicates)\n",
        "        - Requires acks=\"all\" and max_in_flight_requests_per_connection <= 5\n",
        "        - Prevents duplicate messages even if producer retries\n",
        "        \n",
        "        max_in_flight_requests_per_connection:\n",
        "        - Maximum number of unacknowledged requests per connection\n",
        "        - Lower values improve ordering guarantees\n",
        "        - Must be <= 5 when idempotence is enabled\n",
        "        \n",
        "        compression_type:\n",
        "        - \"none\", \"gzip\", \"snappy\", \"lz4\", \"zstd\"\n",
        "        - Reduces network bandwidth and storage\n",
        "        - Snappy is a good balance of speed and compression ratio\n",
        "        \"\"\"\n",
        "        producer_config = {\n",
        "            'bootstrap.servers': self.bootstrap_servers,\n",
        "            \n",
        "            # Reliability settings\n",
        "            'acks': acks,  # Wait for acknowledgment from brokers\n",
        "            'retries': retries,  # Retry failed sends\n",
        "            'retry.backoff.ms': 100,  # Wait 100ms between retries\n",
        "            \n",
        "            # Exactly-once semantics\n",
        "            'enable.idempotence': enable_idempotence,  # Prevent duplicates\n",
        "            \n",
        "            # Performance settings\n",
        "            'max.in.flight.requests.per.connection': max_in_flight_requests_per_connection,\n",
        "            'compression.type': compression_type,  # Compress messages\n",
        "            \n",
        "            # Batching (improves throughput)\n",
        "            'batch.size': 16384,  # Batch size in bytes (16KB)\n",
        "            'linger.ms': 10,  # Wait up to 10ms to fill batch\n",
        "            \n",
        "            # Buffer settings\n",
        "            'buffer.memory': 33554432,  # 32MB buffer for unsent messages\n",
        "            \n",
        "            # Error handling\n",
        "            'max.block.ms': 60000,  # Max time to block when buffer is full\n",
        "        }\n",
        "        \n",
        "        producer = Producer(producer_config)\n",
        "        logger.info(\"Kafka Producer created with configuration:\")\n",
        "        logger.info(f\"  - Bootstrap servers: {self.bootstrap_servers}\")\n",
        "        logger.info(f\"  - Acks: {acks}\")\n",
        "        logger.info(f\"  - Idempotence: {enable_idempotence}\")\n",
        "        logger.info(f\"  - Compression: {compression_type}\")\n",
        "        \n",
        "        return producer\n",
        "    \n",
        "    def produce_messages(\n",
        "        self,\n",
        "        topic: str,\n",
        "        messages: List[Dict[str, Any]],\n",
        "        key_field: Optional[str] = None,\n",
        "        partition: Optional[int] = None,\n",
        "        callback: Optional[Callable] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Produce messages to Kafka topic.\n",
        "        \n",
        "        Args:\n",
        "            topic: Topic name to produce to\n",
        "            messages: List of message dictionaries\n",
        "            key_field: Field name to use as message key (for partitioning)\n",
        "            partition: Specific partition to write to (None = auto-partition)\n",
        "            callback: Optional callback function for delivery reports\n",
        "            \n",
        "        Message Key Importance:\n",
        "        - Messages with same key go to same partition (ensures ordering)\n",
        "        - Useful for maintaining order of related messages\n",
        "        - If None, messages are round-robin distributed across partitions\n",
        "        \n",
        "        Returns:\n",
        "            Statistics about produced messages\n",
        "        \"\"\"\n",
        "        producer = self.create_producer()\n",
        "        \n",
        "        produced_count = 0\n",
        "        failed_count = 0\n",
        "        \n",
        "        def delivery_callback(err, msg):\n",
        "            \"\"\"\n",
        "            Callback function called when message delivery is confirmed.\n",
        "            \n",
        "            This callback is invoked for every message, whether successful or failed.\n",
        "            It's useful for:\n",
        "            - Tracking delivery status\n",
        "            - Logging errors\n",
        "            - Updating metrics\n",
        "            \"\"\"\n",
        "            nonlocal produced_count, failed_count\n",
        "            \n",
        "            if err is not None:\n",
        "                # Message delivery failed\n",
        "                failed_count += 1\n",
        "                error_msg = f\"Message delivery failed: {err}\"\n",
        "                logger.error(error_msg)\n",
        "                self.stats['errors'].append(error_msg)\n",
        "            else:\n",
        "                # Message successfully delivered\n",
        "                produced_count += 1\n",
        "                # msg contains: topic, partition, offset, key, value\n",
        "                if produced_count % 100 == 0:  # Log every 100 messages\n",
        "                    logger.debug(f\"Produced message to {msg.topic()}[{msg.partition()}]@{msg.offset()}\")\n",
        "            \n",
        "            # Call user-provided callback if exists\n",
        "            if callback:\n",
        "                callback(err, msg)\n",
        "        \n",
        "        # Produce each message\n",
        "        for message in messages:\n",
        "            try:\n",
        "                # Serialize message value to JSON bytes\n",
        "                # In production, you might use Avro or Protobuf for better performance\n",
        "                value = json.dumps(message).encode('utf-8')\n",
        "                \n",
        "                # Extract key if specified\n",
        "                key = None\n",
        "                if key_field and key_field in message:\n",
        "                    # Keys are used for partitioning - same key = same partition\n",
        "                    key = str(message[key_field]).encode('utf-8')\n",
        "                \n",
        "                # Produce message asynchronously\n",
        "                # The producer batches messages and sends them in the background\n",
        "                producer.produce(\n",
        "                    topic=topic,\n",
        "                    value=value,  # Message payload\n",
        "                    key=key,  # Optional partition key\n",
        "                    partition=partition,  # Optional specific partition\n",
        "                    callback=delivery_callback  # Called when delivery confirmed\n",
        "                )\n",
        "                \n",
        "                # Poll to trigger delivery callbacks\n",
        "                # This is important - callbacks are only invoked during poll()\n",
        "                producer.poll(0)\n",
        "                \n",
        "            except BufferError:\n",
        "                # Producer buffer is full - wait and retry\n",
        "                logger.warning(\"Producer buffer full, waiting...\")\n",
        "                producer.poll(1)  # Wait for some messages to be sent\n",
        "                # Retry producing this message\n",
        "                producer.produce(topic, value=value, key=key, callback=delivery_callback)\n",
        "            except Exception as e:\n",
        "                failed_count += 1\n",
        "                error_msg = f\"Error producing message: {str(e)}\"\n",
        "                logger.error(error_msg)\n",
        "                self.stats['errors'].append(error_msg)\n",
        "        \n",
        "        # Flush remaining messages\n",
        "        # This blocks until all messages are sent and acknowledged\n",
        "        # Important: Always flush before closing producer\n",
        "        remaining = producer.flush(timeout=30)\n",
        "        \n",
        "        if remaining > 0:\n",
        "            logger.warning(f\"{remaining} messages were not delivered\")\n",
        "        \n",
        "        self.stats['messages_produced'] += produced_count\n",
        "        \n",
        "        return {\n",
        "            'success': True,\n",
        "            'produced': produced_count,\n",
        "            'failed': failed_count,\n",
        "            'topic': topic\n",
        "        }\n",
        "\n",
        "# Initialize Kafka ingestion\n",
        "kafka_ingestion = KafkaIngestion()\n",
        "print(\"✓ Kafka ingestion class initialized (Part 1: Producer)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    # ========================================================================\n",
        "    # KAFKA CONSUMER METHODS\n",
        "    # ========================================================================\n",
        "    \n",
        "    def create_consumer(\n",
        "        self,\n",
        "        group_id: str,\n",
        "        auto_offset_reset: str = \"earliest\",\n",
        "        enable_auto_commit: bool = False,\n",
        "        max_poll_records: int = 500,\n",
        "        session_timeout_ms: int = 30000,\n",
        "        heartbeat_interval_ms: int = 3000\n",
        "    ) -> Consumer:\n",
        "        \"\"\"\n",
        "        Create and configure Kafka Consumer.\n",
        "        \n",
        "        Consumer Configuration Explained:\n",
        "        \n",
        "        group_id:\n",
        "        - Consumers with same group_id form a consumer group\n",
        "        - Partitions are distributed among consumers in the group\n",
        "        - Each partition is consumed by only one consumer in the group\n",
        "        - Different groups can independently consume the same topic\n",
        "        \n",
        "        auto_offset_reset:\n",
        "        - \"earliest\": Start from beginning if no committed offset exists\n",
        "        - \"latest\": Start from end (only new messages)\n",
        "        - \"none\": Throw exception if no offset found\n",
        "        \n",
        "        enable_auto_commit:\n",
        "        - True: Automatically commit offsets periodically\n",
        "        - False: Manual commit (recommended for exactly-once processing)\n",
        "        \n",
        "        max_poll_records:\n",
        "        - Maximum number of records returned per poll()\n",
        "        - Larger values = better throughput but more memory\n",
        "        - Smaller values = lower latency\n",
        "        \n",
        "        session_timeout_ms:\n",
        "        - Time before broker considers consumer dead\n",
        "        - If exceeded, consumer is removed from group and partitions rebalanced\n",
        "        \n",
        "        heartbeat_interval_ms:\n",
        "        - How often consumer sends heartbeat to broker\n",
        "        - Must be < session_timeout_ms / 3\n",
        "        \"\"\"\n",
        "        consumer_config = {\n",
        "            'bootstrap.servers': self.bootstrap_servers,\n",
        "            \n",
        "            # Consumer group settings\n",
        "            'group.id': group_id,  # Consumer group identifier\n",
        "            \n",
        "            # Offset management\n",
        "            'auto.offset.reset': auto_offset_reset,  # What to do if no offset\n",
        "            'enable.auto.commit': enable_auto_commit,  # Auto-commit offsets\n",
        "            \n",
        "            # Polling settings\n",
        "            'max.poll.records': max_poll_records,  # Max records per poll\n",
        "            \n",
        "            # Session and heartbeat\n",
        "            'session.timeout.ms': session_timeout_ms,  # Session timeout\n",
        "            'heartbeat.interval.ms': heartbeat_interval_ms,  # Heartbeat frequency\n",
        "            \n",
        "            # Fetch settings (performance tuning)\n",
        "            'fetch.min.bytes': 1,  # Minimum bytes to fetch\n",
        "            'fetch.max.wait.ms': 500,  # Max wait time for fetch\n",
        "            \n",
        "            # Isolation level (for transactions)\n",
        "            'isolation.level': 'read_committed',  # Only read committed messages\n",
        "        }\n",
        "        \n",
        "        consumer = Consumer(consumer_config)\n",
        "        logger.info(\"Kafka Consumer created with configuration:\")\n",
        "        logger.info(f\"  - Bootstrap servers: {self.bootstrap_servers}\")\n",
        "        logger.info(f\"  - Group ID: {group_id}\")\n",
        "        logger.info(f\"  - Auto offset reset: {auto_offset_reset}\")\n",
        "        logger.info(f\"  - Auto commit: {enable_auto_commit}\")\n",
        "        \n",
        "        return consumer\n",
        "    \n",
        "    def consume_and_write_parquet(\n",
        "        self,\n",
        "        topics: List[str],\n",
        "        group_id: str,\n",
        "        output_path: Optional[str] = None,\n",
        "        batch_size: int = 10000,\n",
        "        timeout: float = 1.0,\n",
        "        max_messages: Optional[int] = None,\n",
        "        commit_after_batch: bool = True\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Consume messages from Kafka and write to Parquet files in batches.\n",
        "        \n",
        "        This method demonstrates:\n",
        "        - Consumer group pattern (parallel processing)\n",
        "        - Batch processing (efficient writes)\n",
        "        - Offset management (resume capability)\n",
        "        - Error handling (skip bad messages)\n",
        "        \n",
        "        Args:\n",
        "            topics: List of topics to subscribe to\n",
        "            group_id: Consumer group ID\n",
        "            output_path: Base path for output files (auto-generated if None)\n",
        "            batch_size: Number of messages per Parquet file\n",
        "            timeout: Poll timeout in seconds (how long to wait for messages)\n",
        "            max_messages: Maximum total messages to consume (None = unlimited)\n",
        "            commit_after_batch: Whether to commit offsets after each batch\n",
        "            \n",
        "        Returns:\n",
        "            Statistics about consumption\n",
        "        \"\"\"\n",
        "        consumer = self.create_consumer(group_id=group_id)\n",
        "        \n",
        "        # Subscribe to topics\n",
        "        # Consumer will automatically join the consumer group\n",
        "        # Partitions will be assigned by the broker (rebalancing)\n",
        "        consumer.subscribe(topics)\n",
        "        logger.info(f\"Subscribed to topics: {topics} with group_id: {group_id}\")\n",
        "        \n",
        "        # Initialize tracking variables\n",
        "        records = []  # Buffer for current batch\n",
        "        batch_count = 0\n",
        "        total_records = 0\n",
        "        start_time = datetime.utcnow()\n",
        "        output_files = []\n",
        "        \n",
        "        # Generate output path if not provided\n",
        "        if output_path is None:\n",
        "            timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            output_path = self.output_dir / f\"kafka_{group_id}_{timestamp}\"\n",
        "        \n",
        "        output_path = Path(output_path)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        try:\n",
        "            logger.info(\"Starting to consume messages...\")\n",
        "            \n",
        "            # Main consumption loop\n",
        "            while True:\n",
        "                # Poll for messages\n",
        "                # Returns immediately if messages available, otherwise waits up to timeout\n",
        "                msg = consumer.poll(timeout=timeout)\n",
        "                \n",
        "                # Check if we've reached max messages limit\n",
        "                if max_messages and total_records >= max_messages:\n",
        "                    logger.info(f\"Reached max_messages limit: {max_messages}\")\n",
        "                    break\n",
        "                \n",
        "                if msg is None:\n",
        "                    # No message received within timeout\n",
        "                    # Write any buffered records before continuing\n",
        "                    if records:\n",
        "                        file_path = self._write_batch_to_parquet(\n",
        "                            records, output_path, batch_count\n",
        "                        )\n",
        "                        output_files.append(file_path)\n",
        "                        \n",
        "                        # Commit offsets after batch (if enabled)\n",
        "                        if commit_after_batch:\n",
        "                            consumer.commit(asynchronous=False)  # Synchronous commit\n",
        "                        \n",
        "                        records = []\n",
        "                        batch_count += 1\n",
        "                    continue\n",
        "                \n",
        "                # Check for errors\n",
        "                if msg.error():\n",
        "                    if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "                        # End of partition reached (normal condition)\n",
        "                        # This happens when we've consumed all messages in a partition\n",
        "                        logger.debug(f\"Reached end of partition: {msg.topic()}[{msg.partition()}]\")\n",
        "                        continue\n",
        "                    else:\n",
        "                        # Actual error occurred\n",
        "                        error_msg = f\"Consumer error: {msg.error()}\"\n",
        "                        logger.error(error_msg)\n",
        "                        self.stats['errors'].append(error_msg)\n",
        "                        break\n",
        "                \n",
        "                # Deserialize and process message\n",
        "                try:\n",
        "                    record = self._deserialize_message(msg)\n",
        "                    records.append(record)\n",
        "                    total_records += 1\n",
        "                    \n",
        "                    # Write batch when size reached\n",
        "                    if len(records) >= batch_size:\n",
        "                        file_path = self._write_batch_to_parquet(\n",
        "                            records, output_path, batch_count\n",
        "                        )\n",
        "                        output_files.append(file_path)\n",
        "                        \n",
        "                        # Commit offsets after batch\n",
        "                        if commit_after_batch:\n",
        "                            consumer.commit(asynchronous=False)\n",
        "                        \n",
        "                        logger.info(f\"Wrote batch {batch_count} with {len(records)} records\")\n",
        "                        records = []\n",
        "                        batch_count += 1\n",
        "                        self.stats['batches_written'] += 1\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    # Error processing individual message\n",
        "                    # Log but continue processing (don't stop entire ingestion)\n",
        "                    error_msg = f\"Error processing message from {msg.topic()}[{msg.partition()}]@{msg.offset()}: {str(e)}\"\n",
        "                    logger.error(error_msg)\n",
        "                    self.stats['errors'].append(error_msg)\n",
        "                    continue\n",
        "        \n",
        "        except KeyboardInterrupt:\n",
        "            logger.info(\"Consumption interrupted by user\")\n",
        "        finally:\n",
        "            # Write any remaining records\n",
        "            if records:\n",
        "                file_path = self._write_batch_to_parquet(\n",
        "                    records, output_path, batch_count\n",
        "                )\n",
        "                output_files.append(file_path)\n",
        "                if commit_after_batch:\n",
        "                    consumer.commit(asynchronous=False)\n",
        "            \n",
        "            # Close consumer\n",
        "            # This triggers a rebalance in the consumer group\n",
        "            consumer.close()\n",
        "            \n",
        "            end_time = datetime.utcnow()\n",
        "            duration = (end_time - start_time).total_seconds()\n",
        "            \n",
        "            self.stats['messages_consumed'] += total_records\n",
        "            \n",
        "            return {\n",
        "                'success': True,\n",
        "                'total_records': total_records,\n",
        "                'batches_written': batch_count + (1 if records else 0),\n",
        "                'output_files': output_files,\n",
        "                'duration_seconds': duration,\n",
        "                'output_path': str(output_path),\n",
        "                'group_id': group_id\n",
        "            }\n",
        "    \n",
        "    def _deserialize_message(self, msg) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Deserialize Kafka message.\n",
        "        \n",
        "        Messages in Kafka are stored as bytes. This method:\n",
        "        1. Decodes bytes to string\n",
        "        2. Parses JSON (or other format)\n",
        "        3. Adds Kafka metadata (topic, partition, offset, timestamp)\n",
        "        \n",
        "        In production, you might use:\n",
        "        - Avro deserializer (with Schema Registry)\n",
        "        - Protobuf deserializer\n",
        "        - Custom binary formats\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Decode message value from bytes to string\n",
        "            value_str = msg.value().decode('utf-8')\n",
        "            \n",
        "            # Parse JSON\n",
        "            # In production, check message headers for content-type\n",
        "            value = json.loads(value_str)\n",
        "            \n",
        "            # Add Kafka metadata for lineage tracking\n",
        "            # This helps track where data came from and when\n",
        "            record = {\n",
        "                **value,  # Original message content\n",
        "                '_kafka_topic': msg.topic(),  # Topic name\n",
        "                '_kafka_partition': msg.partition(),  # Partition number\n",
        "                '_kafka_offset': msg.offset(),  # Offset (position in partition)\n",
        "                '_kafka_timestamp': datetime.utcnow().isoformat(),  # When we processed it\n",
        "            }\n",
        "            \n",
        "            # Add message key if present\n",
        "            if msg.key() is not None:\n",
        "                try:\n",
        "                    record['_kafka_key'] = msg.key().decode('utf-8')\n",
        "                except:\n",
        "                    record['_kafka_key'] = str(msg.key())\n",
        "            \n",
        "            # Add message headers if present\n",
        "            # Headers are key-value pairs attached to messages\n",
        "            if msg.headers():\n",
        "                headers_dict = {}\n",
        "                for header_key, header_value in msg.headers():\n",
        "                    try:\n",
        "                        headers_dict[header_key] = header_value.decode('utf-8')\n",
        "                    except:\n",
        "                        headers_dict[header_key] = str(header_value)\n",
        "                record['_kafka_headers'] = headers_dict\n",
        "            \n",
        "            return record\n",
        "            \n",
        "        except json.JSONDecodeError:\n",
        "            # If not JSON, store as raw string\n",
        "            logger.warning(f\"Message is not JSON, storing as raw string\")\n",
        "            return {\n",
        "                '_raw_message': msg.value().decode('utf-8', errors='ignore'),\n",
        "                '_kafka_topic': msg.topic(),\n",
        "                '_kafka_partition': msg.partition(),\n",
        "                '_kafka_offset': msg.offset(),\n",
        "                '_kafka_timestamp': datetime.utcnow().isoformat()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error deserializing message: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    def _write_batch_to_parquet(\n",
        "        self,\n",
        "        records: List[Dict[str, Any]],\n",
        "        output_path: Path,\n",
        "        batch_number: int\n",
        "    ) -> Path:\n",
        "        \"\"\"\n",
        "        Write batch of records to Parquet file.\n",
        "        \n",
        "        Parquet is ideal for analytics workloads:\n",
        "        - Columnar storage (efficient for column-based queries)\n",
        "        - Compression (saves storage)\n",
        "        - Schema preservation (data types maintained)\n",
        "        - Partitioning support (for time-based queries)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert to DataFrame\n",
        "            df = pd.DataFrame(records)\n",
        "            \n",
        "            # Generate filename with batch number and timestamp\n",
        "            timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"batch_{batch_number:06d}_{timestamp}.parquet\"\n",
        "            file_path = output_path / filename\n",
        "            \n",
        "            # Write to Parquet with optimizations\n",
        "            df.to_parquet(\n",
        "                file_path,\n",
        "                engine='pyarrow',  # PyArrow engine (fastest)\n",
        "                compression='snappy',  # Snappy compression (good balance)\n",
        "                index=False,  # Don't write DataFrame index\n",
        "                schema=self._infer_schema(df)  # Preserve schema\n",
        "            )\n",
        "            \n",
        "            logger.debug(f\"Wrote {len(records)} records to {file_path}\")\n",
        "            return file_path\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error writing Parquet file: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    def _infer_schema(self, df: pd.DataFrame) -> Optional[pa.Schema]:\n",
        "        \"\"\"Infer PyArrow schema from DataFrame.\"\"\"\n",
        "        try:\n",
        "            return pa.Schema.from_pandas(df)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "# Add methods to the class (continuing from previous cell)\n",
        "# Note: In a real notebook, these would be in the same class definition\n",
        "print(\"✓ Kafka Consumer methods defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    # ========================================================================\n",
        "    # ADVANCED KAFKA PATTERNS\n",
        "    # ========================================================================\n",
        "    \n",
        "    def consume_with_exactly_once_semantics(\n",
        "        self,\n",
        "        topics: List[str],\n",
        "        group_id: str,\n",
        "        output_path: Path,\n",
        "        batch_size: int = 1000,\n",
        "        transaction_id: Optional[str] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Consume messages with exactly-once semantics using transactions.\n",
        "        \n",
        "        Exactly-once semantics ensures:\n",
        "        - No duplicate processing\n",
        "        - No lost messages\n",
        "        - Atomic writes (all or nothing)\n",
        "        \n",
        "        Requirements:\n",
        "        - Consumer must have isolation.level='read_committed'\n",
        "        - Producer must have enable.idempotence=True\n",
        "        - Transactional producer must be used\n",
        "        - Requires Kafka 0.11.0+ and broker configuration\n",
        "        \n",
        "        This pattern is useful when:\n",
        "        - Processing financial transactions\n",
        "        - Maintaining exactly-once guarantees\n",
        "        - Writing to multiple systems atomically\n",
        "        \"\"\"\n",
        "        # Create transactional producer\n",
        "        producer_config = {\n",
        "            'bootstrap.servers': self.bootstrap_servers,\n",
        "            'transactional.id': transaction_id or f\"{group_id}-producer\",\n",
        "            'enable.idempotence': True,\n",
        "            'acks': 'all'\n",
        "        }\n",
        "        producer = Producer(producer_config)\n",
        "        \n",
        "        # Initialize transaction\n",
        "        producer.init_transactions()\n",
        "        \n",
        "        # Create consumer with read_committed isolation\n",
        "        consumer_config = {\n",
        "            'bootstrap.servers': self.bootstrap_servers,\n",
        "            'group.id': group_id,\n",
        "            'isolation.level': 'read_committed',  # Only read committed messages\n",
        "            'enable.auto.commit': False  # Manual commit required\n",
        "        }\n",
        "        consumer = Consumer(consumer_config)\n",
        "        consumer.subscribe(topics)\n",
        "        \n",
        "        records = []\n",
        "        total_processed = 0\n",
        "        \n",
        "        try:\n",
        "            while True:\n",
        "                msg = consumer.poll(timeout=1.0)\n",
        "                \n",
        "                if msg is None:\n",
        "                    continue\n",
        "                \n",
        "                if msg.error():\n",
        "                    continue\n",
        "                \n",
        "                # Deserialize message\n",
        "                record = self._deserialize_message(msg)\n",
        "                records.append(record)\n",
        "                \n",
        "                # Process batch\n",
        "                if len(records) >= batch_size:\n",
        "                    # Begin transaction\n",
        "                    producer.begin_transaction()\n",
        "                    \n",
        "                    try:\n",
        "                        # Write to Parquet (your processing)\n",
        "                        self._write_batch_to_parquet(records, output_path, total_processed)\n",
        "                        \n",
        "                        # Send to another topic if needed (example)\n",
        "                        # for record in records:\n",
        "                        #     producer.produce('output_topic', value=json.dumps(record).encode())\n",
        "                        \n",
        "                        # Commit transaction (atomic operation)\n",
        "                        # This commits both the consumer offset and producer messages\n",
        "                        producer.send_offsets_to_transaction(\n",
        "                            consumer.position(consumer.assignment()),\n",
        "                            consumer.consumer_group_metadata()\n",
        "                        )\n",
        "                        producer.commit_transaction()\n",
        "                        \n",
        "                        total_processed += len(records)\n",
        "                        records = []\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        # Abort transaction on error\n",
        "                        producer.abort_transaction()\n",
        "                        logger.error(f\"Transaction aborted: {str(e)}\")\n",
        "                        raise\n",
        "        \n",
        "        finally:\n",
        "            consumer.close()\n",
        "            producer.flush()\n",
        "        \n",
        "        return {'success': True, 'processed': total_processed}\n",
        "    \n",
        "    def consume_with_backpressure_handling(\n",
        "        self,\n",
        "        topics: List[str],\n",
        "        group_id: str,\n",
        "        max_buffer_size: int = 50000,\n",
        "        pause_threshold: float = 0.8,\n",
        "        resume_threshold: float = 0.5\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Handle backpressure when downstream processing is slow.\n",
        "        \n",
        "        Backpressure occurs when:\n",
        "        - Consumer reads faster than processing\n",
        "        - Memory fills up with unprocessed messages\n",
        "        - System becomes unstable\n",
        "        \n",
        "        Solution:\n",
        "        - Pause consumption when buffer is too full\n",
        "        - Resume when buffer is manageable\n",
        "        - Prevents memory exhaustion\n",
        "        \"\"\"\n",
        "        consumer = self.create_consumer(group_id=group_id)\n",
        "        consumer.subscribe(topics)\n",
        "        \n",
        "        buffer = deque(maxlen=max_buffer_size)\n",
        "        paused = False\n",
        "        \n",
        "        try:\n",
        "            while True:\n",
        "                msg = consumer.poll(timeout=1.0)\n",
        "                \n",
        "                if msg is None:\n",
        "                    continue\n",
        "                \n",
        "                if msg.error():\n",
        "                    continue\n",
        "                \n",
        "                # Check buffer size\n",
        "                buffer_usage = len(buffer) / max_buffer_size\n",
        "                \n",
        "                # Pause consumption if buffer is too full\n",
        "                if buffer_usage >= pause_threshold and not paused:\n",
        "                    logger.warning(f\"Buffer usage {buffer_usage:.2%}, pausing consumption\")\n",
        "                    # Pause all assigned partitions\n",
        "                    partitions = consumer.assignment()\n",
        "                    consumer.pause(partitions)\n",
        "                    paused = True\n",
        "                \n",
        "                # Resume consumption when buffer is manageable\n",
        "                elif buffer_usage <= resume_threshold and paused:\n",
        "                    logger.info(f\"Buffer usage {buffer_usage:.2%}, resuming consumption\")\n",
        "                    partitions = consumer.assignment()\n",
        "                    consumer.resume(partitions)\n",
        "                    paused = False\n",
        "                \n",
        "                # Add to buffer\n",
        "                if len(buffer) < max_buffer_size:\n",
        "                    buffer.append(msg)\n",
        "                else:\n",
        "                    logger.warning(\"Buffer full, dropping message\")\n",
        "        \n",
        "        finally:\n",
        "            consumer.close()\n",
        "    \n",
        "    def seek_to_offset(\n",
        "        self,\n",
        "        topic: str,\n",
        "        partition: int,\n",
        "        offset: int,\n",
        "        group_id: str\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Seek consumer to specific offset (replay messages from a point).\n",
        "        \n",
        "        Useful for:\n",
        "        - Reprocessing messages after bug fix\n",
        "        - Starting from specific point in time\n",
        "        - Debugging specific messages\n",
        "        \n",
        "        Note: This only works if consumer is assigned to the partition\n",
        "        \"\"\"\n",
        "        consumer = self.create_consumer(group_id=group_id)\n",
        "        consumer.subscribe([topic])\n",
        "        \n",
        "        # Wait for partition assignment\n",
        "        while True:\n",
        "            partitions = consumer.assignment()\n",
        "            if partitions:\n",
        "                break\n",
        "            consumer.poll(timeout=1.0)\n",
        "        \n",
        "        # Seek to specific offset\n",
        "        from confluent_kafka import TopicPartition\n",
        "        tp = TopicPartition(topic, partition, offset)\n",
        "        consumer.seek(tp)\n",
        "        \n",
        "        logger.info(f\"Seeked to {topic}[{partition}]@{offset}\")\n",
        "        \n",
        "        # Now consume from this offset\n",
        "        # ... consumption logic ...\n",
        "        \n",
        "        consumer.close()\n",
        "    \n",
        "    def get_topic_metadata(self, topic: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get metadata about a Kafka topic.\n",
        "        \n",
        "        Returns:\n",
        "            Information about partitions, leaders, replicas, etc.\n",
        "        \"\"\"\n",
        "        admin_client = AdminClient({'bootstrap.servers': self.bootstrap_servers})\n",
        "        \n",
        "        # Get cluster metadata\n",
        "        metadata = admin_client.list_topics(timeout=10)\n",
        "        \n",
        "        if topic in metadata.topics:\n",
        "            topic_metadata = metadata.topics[topic]\n",
        "            return {\n",
        "                'topic': topic,\n",
        "                'partitions': len(topic_metadata.partitions),\n",
        "                'partition_details': {\n",
        "                    p: {\n",
        "                        'leader': topic_metadata.partitions[p].leader,\n",
        "                        'replicas': topic_metadata.partitions[p].replicas\n",
        "                    }\n",
        "                    for p in topic_metadata.partitions\n",
        "                }\n",
        "            }\n",
        "        else:\n",
        "            return {'error': f'Topic {topic} not found'}\n",
        "\n",
        "# Complete the class definition\n",
        "print(\"✓ Advanced Kafka patterns defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kafka Usage Examples\n",
        "\n",
        "Let's demonstrate Kafka ingestion with practical examples. Note: These examples require a running Kafka cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# EXAMPLE 1: Produce Messages to Kafka Topic\n",
        "# ========================================================================\n",
        "\n",
        "# Sample data to produce\n",
        "sample_messages = [\n",
        "    {\n",
        "        'customer_id': 1001,\n",
        "        'event_type': 'purchase',\n",
        "        'amount': 99.99,\n",
        "        'timestamp': datetime.utcnow().isoformat()\n",
        "    },\n",
        "    {\n",
        "        'customer_id': 1002,\n",
        "        'event_type': 'view',\n",
        "        'product_id': 'PROD123',\n",
        "        'timestamp': datetime.utcnow().isoformat()\n",
        "    },\n",
        "    {\n",
        "        'customer_id': 1003,\n",
        "        'event_type': 'purchase',\n",
        "        'amount': 149.50,\n",
        "        'timestamp': datetime.utcnow().isoformat()\n",
        "    }\n",
        "]\n",
        "\n",
        "# Produce messages\n",
        "# Note: This requires a running Kafka broker\n",
        "# Uncomment to run:\n",
        "# result = kafka_ingestion.produce_messages(\n",
        "#     topic='customer-events',\n",
        "#     messages=sample_messages,\n",
        "#     key_field='customer_id'  # Messages with same customer_id go to same partition\n",
        "# )\n",
        "# print(f\"Produced {result['produced']} messages, {result['failed']} failed\")\n",
        "\n",
        "print(\"Example 1: Message production code ready\")\n",
        "print(\"Note: Requires running Kafka broker to execute\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# EXAMPLE 2: Consume Messages and Write to Parquet\n",
        "# ========================================================================\n",
        "\n",
        "# Consume messages from Kafka and write to Parquet files\n",
        "# This demonstrates the complete ingestion pipeline\n",
        "\n",
        "# Uncomment to run:\n",
        "# result = kafka_ingestion.consume_and_write_parquet(\n",
        "#     topics=['customer-events', 'order-events'],  # Subscribe to multiple topics\n",
        "#     group_id='bronze-ingestion-group',  # Consumer group ID\n",
        "#     batch_size=1000,  # Write Parquet file every 1000 messages\n",
        "#     timeout=5.0,  # Wait up to 5 seconds for messages\n",
        "#     max_messages=10000,  # Stop after 10000 messages (None for unlimited)\n",
        "#     commit_after_batch=True  # Commit offsets after each batch\n",
        "# )\n",
        "# \n",
        "# print(f\"Consumed {result['total_records']} records\")\n",
        "# print(f\"Wrote {result['batches_written']} batches\")\n",
        "# print(f\"Output files: {result['output_files']}\")\n",
        "\n",
        "print(\"Example 2: Message consumption code ready\")\n",
        "print(\"Note: Requires running Kafka broker to execute\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# EXAMPLE 3: Consumer Group Pattern (Parallel Processing)\n",
        "# ========================================================================\n",
        "\n",
        "# Consumer groups allow multiple consumers to work together:\n",
        "# - Each partition is consumed by only one consumer in the group\n",
        "# - Adding more consumers increases parallelism (up to number of partitions)\n",
        "# - If a consumer fails, its partitions are reassigned to other consumers\n",
        "\n",
        "def run_consumer_instance(instance_id: int, topics: List[str], group_id: str):\n",
        "    \"\"\"\n",
        "    Run a single consumer instance.\n",
        "    \n",
        "    In production, you would run multiple instances of this function\n",
        "    (e.g., in separate processes or containers) to achieve parallelism.\n",
        "    \"\"\"\n",
        "    consumer = kafka_ingestion.create_consumer(group_id=group_id)\n",
        "    consumer.subscribe(topics)\n",
        "    \n",
        "    print(f\"Consumer instance {instance_id} started\")\n",
        "    \n",
        "    try:\n",
        "        while True:\n",
        "            msg = consumer.poll(timeout=1.0)\n",
        "            \n",
        "            if msg is None:\n",
        "                continue\n",
        "            \n",
        "            if msg.error():\n",
        "                continue\n",
        "            \n",
        "            # Process message\n",
        "            record = kafka_ingestion._deserialize_message(msg)\n",
        "            print(f\"Instance {instance_id} processed: {record.get('customer_id', 'N/A')}\")\n",
        "            \n",
        "            # Commit offset manually\n",
        "            consumer.commit(asynchronous=False)\n",
        "    \n",
        "    finally:\n",
        "        consumer.close()\n",
        "\n",
        "# To run multiple consumers in parallel:\n",
        "# import threading\n",
        "# threads = []\n",
        "# for i in range(3):  # 3 consumer instances\n",
        "#     t = threading.Thread(target=run_consumer_instance, args=(i, ['customer-events'], 'my-group'))\n",
        "#     t.start()\n",
        "#     threads.append(t)\n",
        "\n",
        "print(\"Example 3: Consumer group pattern code ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kafka Best Practices and Patterns\n",
        "\n",
        "### 1. **Idempotent Producers**\n",
        "- Enable `enable.idempotence=True` to prevent duplicates\n",
        "- Requires `acks='all'` and `max.in.flight.requests.per.connection <= 5`\n",
        "\n",
        "### 2. **Consumer Groups**\n",
        "- Use unique group IDs for different applications\n",
        "- Scale consumers up to the number of partitions\n",
        "- Monitor consumer lag (how far behind consumers are)\n",
        "\n",
        "### 3. **Offset Management**\n",
        "- Manual commits for exactly-once processing\n",
        "- Store offsets externally for complex workflows\n",
        "- Use `auto.offset.reset` carefully (earliest vs latest)\n",
        "\n",
        "### 4. **Error Handling**\n",
        "- Implement dead letter queues for failed messages\n",
        "- Retry with exponential backoff\n",
        "- Log errors for monitoring\n",
        "\n",
        "### 5. **Performance Tuning**\n",
        "- Batch size: Larger batches = better throughput, higher latency\n",
        "- Compression: Use snappy or zstd for better compression\n",
        "- Partitioning: Choose keys that distribute evenly\n",
        "\n",
        "### 6. **Monitoring**\n",
        "- Track consumer lag (messages behind)\n",
        "- Monitor throughput (messages per second)\n",
        "- Alert on errors and failures\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook covered:\n",
        "1. ✅ File-based ingestion (CSV, JSON, Parquet, Excel)\n",
        "2. ✅ API-based ingestion (REST APIs, pagination)\n",
        "3. ✅ Database ingestion (PostgreSQL, MongoDB)\n",
        "4. ✅ IoT ingestion (MQTT protocol)\n",
        "5. ✅ Kafka ingestion (producers, consumers, advanced patterns)\n",
        "\n",
        "Each method includes:\n",
        "- Detailed inline comments explaining concepts\n",
        "- Error handling and retry logic\n",
        "- Metadata tracking for data lineage\n",
        "- Parquet output format for analytics\n",
        "- Production-ready patterns and best practices\n",
        "\n",
        "**Next Steps:**\n",
        "- Set up a local Kafka cluster for testing\n",
        "- Experiment with different consumer group configurations\n",
        "- Implement schema registry for Avro schemas\n",
        "- Add monitoring and alerting\n",
        "- Scale to production workloads"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
